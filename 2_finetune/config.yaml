# ── MLX-LM LoRA Fine-Tuning Configuration ─────────────────────────────────────
# Step 2 — Fine-tune Llama 3.2 3B Instruct on ChatDoctor data
#
# Run from the project root:
#   python -m mlx_lm.lora --config 2_finetune/config.yaml
#
# Prerequisites:
#   pip install mlx-lm
#   python 1_data/prepare_data.py   ← generates train.jsonl / valid.jsonl first
# ─────────────────────────────────────────────────────────────────────────────

# ── Base Model ────────────────────────────────────────────────────────────────
model: "meta-llama/Llama-3.2-3B-Instruct"

# ── Training Data ─────────────────────────────────────────────────────────────
train: true
data: "1_data/processed"          # folder with train.jsonl and valid.jsonl

# ── Output ────────────────────────────────────────────────────────────────────
adapter_path: "2_finetune/adapters"   # LoRA weights saved here during training

# ── LoRA Hyperparameters ──────────────────────────────────────────────────────
lora_layers: 16           # number of transformer layers to apply LoRA to
lora_parameters:
  rank: 8                 # LoRA rank — higher = more capacity, more VRAM
  alpha: 16               # scaling factor (usually 2x rank)
  dropout: 0.05
  scale: 10.0

# ── Training Hyperparameters ──────────────────────────────────────────────────
batch_size: 4             # safe for M2/M3 16GB; lower to 2 if OOM
iters: 1000               # total training steps (~2-3 hrs on M2)
val_batches: 25           # number of batches to evaluate on
learning_rate: 1e-4
steps_per_report: 10      # print train loss every N steps
steps_per_eval: 100       # evaluate on validation every N steps
save_every: 100           # save adapter checkpoint every N steps
max_seq_length: 2048      # max tokens per example (trim longer ones)
grad_checkpoint: true     # enable gradient checkpointing to save memory
